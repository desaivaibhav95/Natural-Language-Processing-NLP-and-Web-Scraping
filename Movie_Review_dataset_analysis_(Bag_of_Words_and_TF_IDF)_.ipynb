{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Movie Review dataset analysis (Bag of Words and TF-IDF) .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "11T2ds0YRnFSGIRpXQpTeoQddDxkWypEX",
      "authorship_tag": "ABX9TyPyHA8wY/0UWxBPnMivJzcY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/desaivaibhav95/Natural-Language-Processing-NLP-and-Web-Scraping/blob/master/Movie_Review_dataset_analysis_(Bag_of_Words_and_TF_IDF)_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxRHEHfl2vLD",
        "colab_type": "text"
      },
      "source": [
        "The Movie Review dataset consists of two columns class and text. The text column consists of reviews and the class column consists the information of whether the review is positive or negative."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL2vilS9m1W8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/My Drive/NLP Training/movie-Review dataset.csv')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8zzEQs13LL_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "50199586-970a-42c0-ccb2-bee74ff7a06b"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Pos</td>\n",
              "      <td>films adapted from comic books have had plent...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Pos</td>\n",
              "      <td>every now and then a movie comes along from a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Pos</td>\n",
              "      <td>you ve got mail works alot better than it des...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Pos</td>\n",
              "      <td>jaws   is a rare film that grabs your atte...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pos</td>\n",
              "      <td>moviemaking is a lot like being the general m...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  class                                               text\n",
              "0   Pos   films adapted from comic books have had plent...\n",
              "1   Pos   every now and then a movie comes along from a...\n",
              "2   Pos   you ve got mail works alot better than it des...\n",
              "3   Pos      jaws   is a rare film that grabs your atte...\n",
              "4   Pos   moviemaking is a lot like being the general m..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHshos7mnuTU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "np.random.seed(500)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IohoAncgn1if",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "273d435c-6793-44bc-fa69-642403ea73ef"
      },
      "source": [
        "# Remove number\n",
        "import re # import all regular expressions\n",
        "df['text'] = [re.sub('\\d','', i) for i in df['text']]\n",
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Pos</td>\n",
              "      <td>films adapted from comic books have had plent...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Pos</td>\n",
              "      <td>every now and then a movie comes along from a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Pos</td>\n",
              "      <td>you ve got mail works alot better than it des...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Pos</td>\n",
              "      <td>jaws   is a rare film that grabs your atte...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pos</td>\n",
              "      <td>moviemaking is a lot like being the general m...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  class                                               text\n",
              "0   Pos   films adapted from comic books have had plent...\n",
              "1   Pos   every now and then a movie comes along from a...\n",
              "2   Pos   you ve got mail works alot better than it des...\n",
              "3   Pos      jaws   is a rare film that grabs your atte...\n",
              "4   Pos   moviemaking is a lot like being the general m..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vq_xRAj9owaI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "3aff66ee-d590-4adb-a0a1-55793a1410f5"
      },
      "source": [
        "# Replace punctuations with white space\n",
        "import string\n",
        "df['text'] = [re.sub('[%s]' % re.escape(string.punctuation), '', i) for i in df['text']]\n",
        "df.head(10)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Pos</td>\n",
              "      <td>films adapted from comic books have had plent...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Pos</td>\n",
              "      <td>every now and then a movie comes along from a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Pos</td>\n",
              "      <td>you ve got mail works alot better than it des...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Pos</td>\n",
              "      <td>jaws   is a rare film that grabs your atte...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pos</td>\n",
              "      <td>moviemaking is a lot like being the general m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Pos</td>\n",
              "      <td>on june       a self taught   idealistic   ye...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Pos</td>\n",
              "      <td>apparently   director tony kaye had a major b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Pos</td>\n",
              "      <td>one of my colleagues was surprised when i tol...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Pos</td>\n",
              "      <td>after bloody clashes and independence won   l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Pos</td>\n",
              "      <td>the american action film has been slowly drow...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  class                                               text\n",
              "0   Pos   films adapted from comic books have had plent...\n",
              "1   Pos   every now and then a movie comes along from a...\n",
              "2   Pos   you ve got mail works alot better than it des...\n",
              "3   Pos      jaws   is a rare film that grabs your atte...\n",
              "4   Pos   moviemaking is a lot like being the general m...\n",
              "5   Pos   on june       a self taught   idealistic   ye...\n",
              "6   Pos   apparently   director tony kaye had a major b...\n",
              "7   Pos   one of my colleagues was surprised when i tol...\n",
              "8   Pos   after bloody clashes and independence won   l...\n",
              "9   Pos   the american action film has been slowly drow..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4N1KzbNSpUgC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert all uppercase letters in the text to lowercase\n",
        "df['text'] = [i.lower() for i in df['text']] "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35sXPsk5xG7v",
        "colab_type": "text"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZHIAezWpm8G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "5ce0482e-9ee2-40f5-f9e1-a9399e0b8c5a"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "df['text_wt'] = [word_tokenize(i) for i in df['text']]\n",
        "df.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>text</th>\n",
              "      <th>text_wt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Pos</td>\n",
              "      <td>films adapted from comic books have had plent...</td>\n",
              "      <td>[films, adapted, from, comic, books, have, had...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Pos</td>\n",
              "      <td>every now and then a movie comes along from a...</td>\n",
              "      <td>[every, now, and, then, a, movie, comes, along...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Pos</td>\n",
              "      <td>you ve got mail works alot better than it des...</td>\n",
              "      <td>[you, ve, got, mail, works, alot, better, than...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Pos</td>\n",
              "      <td>jaws   is a rare film that grabs your atte...</td>\n",
              "      <td>[jaws, is, a, rare, film, that, grabs, your, a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pos</td>\n",
              "      <td>moviemaking is a lot like being the general m...</td>\n",
              "      <td>[moviemaking, is, a, lot, like, being, the, ge...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  class  ...                                            text_wt\n",
              "0   Pos  ...  [films, adapted, from, comic, books, have, had...\n",
              "1   Pos  ...  [every, now, and, then, a, movie, comes, along...\n",
              "2   Pos  ...  [you, ve, got, mail, works, alot, better, than...\n",
              "3   Pos  ...  [jaws, is, a, rare, film, that, grabs, your, a...\n",
              "4   Pos  ...  [moviemaking, is, a, lot, like, being, the, ge...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyyN-ycmpvfh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f39dacd4-a16b-419a-ab36-0e8ac1a8051f"
      },
      "source": [
        "# To show the stop words\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words # Show stop words"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " 'if',\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s40o_zkd5pdf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "e34eab2a-6227-420a-96ce-58de16955ec5"
      },
      "source": [
        "# Remove all stop words\n",
        "df['text_SW'] = [[i for i in j if not i in stop_words] for j in df['text_wt']] # remove the word which is available\n",
        "# in stopword library\n",
        "df.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>text</th>\n",
              "      <th>text_wt</th>\n",
              "      <th>text_SW</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Pos</td>\n",
              "      <td>films adapted from comic books have had plent...</td>\n",
              "      <td>[films, adapted, from, comic, books, have, had...</td>\n",
              "      <td>[films, adapted, comic, books, plenty, success...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Pos</td>\n",
              "      <td>every now and then a movie comes along from a...</td>\n",
              "      <td>[every, now, and, then, a, movie, comes, along...</td>\n",
              "      <td>[every, movie, comes, along, suspect, studio, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Pos</td>\n",
              "      <td>you ve got mail works alot better than it des...</td>\n",
              "      <td>[you, ve, got, mail, works, alot, better, than...</td>\n",
              "      <td>[got, mail, works, alot, better, deserves, ord...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Pos</td>\n",
              "      <td>jaws   is a rare film that grabs your atte...</td>\n",
              "      <td>[jaws, is, a, rare, film, that, grabs, your, a...</td>\n",
              "      <td>[jaws, rare, film, grabs, attention, shows, si...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pos</td>\n",
              "      <td>moviemaking is a lot like being the general m...</td>\n",
              "      <td>[moviemaking, is, a, lot, like, being, the, ge...</td>\n",
              "      <td>[moviemaking, lot, like, general, manager, nfl...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  class  ...                                            text_SW\n",
              "0   Pos  ...  [films, adapted, comic, books, plenty, success...\n",
              "1   Pos  ...  [every, movie, comes, along, suspect, studio, ...\n",
              "2   Pos  ...  [got, mail, works, alot, better, deserves, ord...\n",
              "3   Pos  ...  [jaws, rare, film, grabs, attention, shows, si...\n",
              "4   Pos  ...  [moviemaking, lot, like, general, manager, nfl...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E9WpXL5xOjq",
        "colab_type": "text"
      },
      "source": [
        "## Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGGldMJZujNr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "fa36d89a-c79a-4a96-dc1e-f2fac869f12c"
      },
      "source": [
        "nltk.download('tagsets')\n",
        "# nltk.help.upenn_tagset()# tagset documentation\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from collections import defaultdict #Default Dictionary is imported from collections\n",
        "from nltk.corpus import wordnet as wn #the corpus reader wordnet is imported.\n",
        "from nltk.tag import pos_tag\n",
        "# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. \n",
        "#By default it is set to Noun\n",
        "tag_map = defaultdict(lambda : wn.NOUN) #Dictionary is created where pos_tag (first letter) are the key values \n",
        "tag_map['J'] = wn.ADJ                   #whose values are mapped with the value \n",
        "tag_map['V'] = wn.VERB                  #from wordnet dictionary. We have taken the only first letter as \n",
        "tag_map['R'] = wn.ADV\n",
        "# we will use it later in the loop.\n",
        "\n",
        "tag_map"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Package tagsets is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(<function __main__.<lambda>>, {'J': 'a', 'R': 'r', 'V': 'v'})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ott_qR3Sv8vu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "f583c503-709b-4d01-d70a-eaa2c7e9c62e"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Lemmatization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Initializing WordNetLemmatizer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "df['lemma'] = [[lemmatizer.lemmatize(word, tag_map[tag[0]]) for word, tag in pos_tag(i)] for i in df['text_SW']]\n",
        "df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>text</th>\n",
              "      <th>text_wt</th>\n",
              "      <th>text_SW</th>\n",
              "      <th>lemma</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Pos</td>\n",
              "      <td>films adapted from comic books have had plent...</td>\n",
              "      <td>[films, adapted, from, comic, books, have, had...</td>\n",
              "      <td>[films, adapted, comic, books, plenty, success...</td>\n",
              "      <td>[film, adapt, comic, book, plenty, success, wh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Pos</td>\n",
              "      <td>every now and then a movie comes along from a...</td>\n",
              "      <td>[every, now, and, then, a, movie, comes, along...</td>\n",
              "      <td>[every, movie, comes, along, suspect, studio, ...</td>\n",
              "      <td>[every, movie, come, along, suspect, studio, e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Pos</td>\n",
              "      <td>you ve got mail works alot better than it des...</td>\n",
              "      <td>[you, ve, got, mail, works, alot, better, than...</td>\n",
              "      <td>[got, mail, works, alot, better, deserves, ord...</td>\n",
              "      <td>[get, mail, work, alot, good, deserves, order,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Pos</td>\n",
              "      <td>jaws   is a rare film that grabs your atte...</td>\n",
              "      <td>[jaws, is, a, rare, film, that, grabs, your, a...</td>\n",
              "      <td>[jaws, rare, film, grabs, attention, shows, si...</td>\n",
              "      <td>[jaw, rare, film, grab, attention, show, singl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pos</td>\n",
              "      <td>moviemaking is a lot like being the general m...</td>\n",
              "      <td>[moviemaking, is, a, lot, like, being, the, ge...</td>\n",
              "      <td>[moviemaking, lot, like, general, manager, nfl...</td>\n",
              "      <td>[moviemaking, lot, like, general, manager, nfl...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  class  ...                                              lemma\n",
              "0   Pos  ...  [film, adapt, comic, book, plenty, success, wh...\n",
              "1   Pos  ...  [every, movie, come, along, suspect, studio, e...\n",
              "2   Pos  ...  [get, mail, work, alot, good, deserves, order,...\n",
              "3   Pos  ...  [jaw, rare, film, grab, attention, show, singl...\n",
              "4   Pos  ...  [moviemaking, lot, like, general, manager, nfl...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iyb6SvqNBICm",
        "colab_type": "text"
      },
      "source": [
        "We can observe in the above dataframe, how our text data has been transformed after applying operations like tokenization, stop words removal, Part of Speech tagging and lemmatization. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLlSzJMawp12",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "d847f9a2-7ffe-4402-e804-d34528b5f674"
      },
      "source": [
        "# Joining all the words in the list of words after applying lemmatization operation\n",
        "\n",
        "df['lemma2'] = df['lemma'].apply(lambda x: ' '.join(x))\n",
        "df['lemma2'].head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    film adapt comic book plenty success whether s...\n",
              "1    every movie come along suspect studio every in...\n",
              "2    get mail work alot good deserves order make fi...\n",
              "3    jaw rare film grab attention show single image...\n",
              "4    moviemaking lot like general manager nfl team ...\n",
              "Name: lemma2, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GRyhDbSxkS3",
        "colab_type": "text"
      },
      "source": [
        "## Word Embedding using Bag of Words (BOW)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zk9CvlbHzBRS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Bag of Words\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# instantiate CountVectorizer()\n",
        "# CountVectorizer to count the number of words (term frequency)\n",
        "cv = CountVectorizer(max_features = 5000)\n",
        "\n",
        "#this steps generates word counts for the words in your docs and return term-document matrix.\n",
        "BOW = cv.fit_transform(df['lemma2']).toarray()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2l7TZGszsV7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "463d3297-433b-4abd-a6f1-44451446f2e7"
      },
      "source": [
        "pd.DataFrame(BOW, columns=cv.get_feature_names()).head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>aaron</th>\n",
              "      <th>abandon</th>\n",
              "      <th>ability</th>\n",
              "      <th>able</th>\n",
              "      <th>aboard</th>\n",
              "      <th>abound</th>\n",
              "      <th>abraham</th>\n",
              "      <th>absence</th>\n",
              "      <th>absent</th>\n",
              "      <th>absolute</th>\n",
              "      <th>absolutely</th>\n",
              "      <th>absorb</th>\n",
              "      <th>absurd</th>\n",
              "      <th>abuse</th>\n",
              "      <th>abusive</th>\n",
              "      <th>abyss</th>\n",
              "      <th>academy</th>\n",
              "      <th>accent</th>\n",
              "      <th>accept</th>\n",
              "      <th>acceptable</th>\n",
              "      <th>acceptance</th>\n",
              "      <th>accepts</th>\n",
              "      <th>accident</th>\n",
              "      <th>accidentally</th>\n",
              "      <th>acclaim</th>\n",
              "      <th>accompany</th>\n",
              "      <th>accomplish</th>\n",
              "      <th>accomplishment</th>\n",
              "      <th>accord</th>\n",
              "      <th>account</th>\n",
              "      <th>accurate</th>\n",
              "      <th>accuse</th>\n",
              "      <th>ace</th>\n",
              "      <th>achieve</th>\n",
              "      <th>achievement</th>\n",
              "      <th>acid</th>\n",
              "      <th>acknowledge</th>\n",
              "      <th>across</th>\n",
              "      <th>act</th>\n",
              "      <th>acting</th>\n",
              "      <th>...</th>\n",
              "      <th>worth</th>\n",
              "      <th>worthless</th>\n",
              "      <th>worthwhile</th>\n",
              "      <th>worthy</th>\n",
              "      <th>would</th>\n",
              "      <th>wound</th>\n",
              "      <th>wow</th>\n",
              "      <th>wrap</th>\n",
              "      <th>wreak</th>\n",
              "      <th>wreck</th>\n",
              "      <th>wrestle</th>\n",
              "      <th>wrestler</th>\n",
              "      <th>wrestling</th>\n",
              "      <th>wright</th>\n",
              "      <th>write</th>\n",
              "      <th>writer</th>\n",
              "      <th>writerdirector</th>\n",
              "      <th>wrong</th>\n",
              "      <th>yard</th>\n",
              "      <th>yawn</th>\n",
              "      <th>yeah</th>\n",
              "      <th>year</th>\n",
              "      <th>yearn</th>\n",
              "      <th>yell</th>\n",
              "      <th>yellow</th>\n",
              "      <th>yes</th>\n",
              "      <th>yet</th>\n",
              "      <th>york</th>\n",
              "      <th>young</th>\n",
              "      <th>youngster</th>\n",
              "      <th>youth</th>\n",
              "      <th>zane</th>\n",
              "      <th>zany</th>\n",
              "      <th>zellweger</th>\n",
              "      <th>zero</th>\n",
              "      <th>zeta</th>\n",
              "      <th>zombie</th>\n",
              "      <th>zone</th>\n",
              "      <th>zoom</th>\n",
              "      <th>zwick</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 5000 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   aaron  abandon  ability  able  aboard  ...  zeta  zombie  zone  zoom  zwick\n",
              "0      0        0        0     0       0  ...     0       0     0     0      0\n",
              "1      0        0        0     0       0  ...     0       0     0     0      0\n",
              "2      0        0        0     0       0  ...     0       0     0     0      0\n",
              "3      0        0        0     0       1  ...     0       0     0     0      0\n",
              "4      0        0        0     0       0  ...     0       0     0     0      0\n",
              "\n",
              "[5 rows x 5000 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EnN3IO61upa",
        "colab_type": "text"
      },
      "source": [
        "## Word Embedding using TF-IDF Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwG2a89G17c9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# In TF-IDF vectorizer all the words do not have the same weightage unlike Part of Speech tagging \n",
        "# The most frequently appearing word (measured by Term Frequency (TF)) has the highest weightage\n",
        "from sklearn. feature_extraction.text import TfidfVectorizer\n",
        "tf = TfidfVectorizer(max_features = 5000)\n",
        "Tfidf = tf.fit_transform(df['lemma2']).toarray()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3MjaSUk17av",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "f53ba0b2-f6d5-483b-d574-0951c01410a4"
      },
      "source": [
        "pd.DataFrame(Tfidf, columns = cv.get_feature_names()).head()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>aaron</th>\n",
              "      <th>abandon</th>\n",
              "      <th>ability</th>\n",
              "      <th>able</th>\n",
              "      <th>aboard</th>\n",
              "      <th>abound</th>\n",
              "      <th>abraham</th>\n",
              "      <th>absence</th>\n",
              "      <th>absent</th>\n",
              "      <th>absolute</th>\n",
              "      <th>absolutely</th>\n",
              "      <th>absorb</th>\n",
              "      <th>absurd</th>\n",
              "      <th>abuse</th>\n",
              "      <th>abusive</th>\n",
              "      <th>abyss</th>\n",
              "      <th>academy</th>\n",
              "      <th>accent</th>\n",
              "      <th>accept</th>\n",
              "      <th>acceptable</th>\n",
              "      <th>acceptance</th>\n",
              "      <th>accepts</th>\n",
              "      <th>accident</th>\n",
              "      <th>accidentally</th>\n",
              "      <th>acclaim</th>\n",
              "      <th>accompany</th>\n",
              "      <th>accomplish</th>\n",
              "      <th>accomplishment</th>\n",
              "      <th>accord</th>\n",
              "      <th>account</th>\n",
              "      <th>accurate</th>\n",
              "      <th>accuse</th>\n",
              "      <th>ace</th>\n",
              "      <th>achieve</th>\n",
              "      <th>achievement</th>\n",
              "      <th>acid</th>\n",
              "      <th>acknowledge</th>\n",
              "      <th>across</th>\n",
              "      <th>act</th>\n",
              "      <th>acting</th>\n",
              "      <th>...</th>\n",
              "      <th>worth</th>\n",
              "      <th>worthless</th>\n",
              "      <th>worthwhile</th>\n",
              "      <th>worthy</th>\n",
              "      <th>would</th>\n",
              "      <th>wound</th>\n",
              "      <th>wow</th>\n",
              "      <th>wrap</th>\n",
              "      <th>wreak</th>\n",
              "      <th>wreck</th>\n",
              "      <th>wrestle</th>\n",
              "      <th>wrestler</th>\n",
              "      <th>wrestling</th>\n",
              "      <th>wright</th>\n",
              "      <th>write</th>\n",
              "      <th>writer</th>\n",
              "      <th>writerdirector</th>\n",
              "      <th>wrong</th>\n",
              "      <th>yard</th>\n",
              "      <th>yawn</th>\n",
              "      <th>yeah</th>\n",
              "      <th>year</th>\n",
              "      <th>yearn</th>\n",
              "      <th>yell</th>\n",
              "      <th>yellow</th>\n",
              "      <th>yes</th>\n",
              "      <th>yet</th>\n",
              "      <th>york</th>\n",
              "      <th>young</th>\n",
              "      <th>youngster</th>\n",
              "      <th>youth</th>\n",
              "      <th>zane</th>\n",
              "      <th>zany</th>\n",
              "      <th>zellweger</th>\n",
              "      <th>zero</th>\n",
              "      <th>zeta</th>\n",
              "      <th>zombie</th>\n",
              "      <th>zone</th>\n",
              "      <th>zoom</th>\n",
              "      <th>zwick</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.10482</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.053954</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.020882</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.015287</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.043991</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.064522</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.061462</td>\n",
              "      <td>0.037455</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.062557</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.045004</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.036018</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.028538</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.083493</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.010771</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.011621</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.02393</td>\n",
              "      <td>0.015487</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.069542</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.03644</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 5000 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   aaron  abandon  ability  able    aboard  ...  zeta  zombie  zone  zoom  zwick\n",
              "0    0.0      0.0      0.0   0.0  0.000000  ...   0.0     0.0   0.0   0.0    0.0\n",
              "1    0.0      0.0      0.0   0.0  0.000000  ...   0.0     0.0   0.0   0.0    0.0\n",
              "2    0.0      0.0      0.0   0.0  0.000000  ...   0.0     0.0   0.0   0.0    0.0\n",
              "3    0.0      0.0      0.0   0.0  0.036018  ...   0.0     0.0   0.0   0.0    0.0\n",
              "4    0.0      0.0      0.0   0.0  0.000000  ...   0.0     0.0   0.0   0.0    0.0\n",
              "\n",
              "[5 rows x 5000 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOumI_pyxuI1",
        "colab_type": "text"
      },
      "source": [
        "## Model Preprocessing (Encoding categorical variables and train test split)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMb2jZvW00UW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "Encoder = LabelEncoder()\n",
        "df['class2'] = Encoder.fit_transform(df['class'])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0hBFvdmx5j6",
        "colab_type": "text"
      },
      "source": [
        "## Model Training and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcVe1fRNDihc",
        "colab_type": "text"
      },
      "source": [
        "Preparing two separate datasets for training for both the word embedding techniques"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVW-8BjjEJc7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxqDWDgdi__Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Bag of Words (Count Vectorizer)\n",
        "Train_X, Test_X, Train_Y, Test_Y = train_test_split(BOW, df['class2'], test_size = 0.2)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njNP9UbF3izT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TF-IDF Vectorizer\n",
        "Train_X1, Test_X1, Train_Y1, Test_Y1 = train_test_split(Tfidf, df['class2'], test_size = 0.2)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4K8NkAsqyAQY",
        "colab_type": "text"
      },
      "source": [
        "### Naive Bayes (BOW)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjmj7x_rj_Lz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8b370948-94a3-49c8-aa47-fe85ada891cc"
      },
      "source": [
        "from sklearn import model_selection, naive_bayes, svm, linear_model\n",
        "from sklearn.metrics import accuracy_score\n",
        "# fit the training dataset on the NB classifier\n",
        "Naive = naive_bayes.MultinomialNB()\n",
        "Naive.fit(Train_X,Train_Y)\n",
        "# predict the labels on validation dataset\n",
        "predictions_NB = Naive.predict(Test_X)\n",
        "# Use accuracy_score function to get the accuracy\n",
        "print(\"Naive Bayes Accuracy Score -> \",round(accuracy_score(predictions_NB, Test_Y)*100,2),\"%\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Naive Bayes Accuracy Score ->  83.25 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEAiFPSukU34",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "1828a8b1-47b0-4bf4-9d41-fb59fca90e37"
      },
      "source": [
        "from sklearn import metrics\n",
        "print(\" ------ Confusion Matrix-----[TN FP  FN TP]\")\n",
        "\n",
        "print(metrics.confusion_matrix(predictions_NB, Test_Y))\n",
        "print(metrics.classification_report(predictions_NB, Test_Y))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " ------ Confusion Matrix-----[TN FP  FN TP]\n",
            "[[167  38]\n",
            " [ 29 166]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.81      0.83       205\n",
            "           1       0.81      0.85      0.83       195\n",
            "\n",
            "    accuracy                           0.83       400\n",
            "   macro avg       0.83      0.83      0.83       400\n",
            "weighted avg       0.83      0.83      0.83       400\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdepaLEF4BSh",
        "colab_type": "text"
      },
      "source": [
        "## Naive Bayes (TF_IDF)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUP9kE_p4GEN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e2db8c15-af8f-4577-8cc2-b3f7f8501ed0"
      },
      "source": [
        "Naive_1 = naive_bayes.MultinomialNB()\n",
        "Naive_1.fit(Train_X1,Train_Y1)\n",
        "# predict the labels on validation dataset\n",
        "predictions_NB_1 = Naive.predict(Test_X1)\n",
        "# Use accuracy_score function to get the accuracy\n",
        "print(\"Naive Bayes Accuracy Score -> \",round(accuracy_score(predictions_NB_1, Test_Y1)*100,2),\"%\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Naive Bayes Accuracy Score ->  87.25 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3Ry535d4F93",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "c4038095-1bda-4176-8712-b0b61ba146a8"
      },
      "source": [
        "print(\" ------ Confusion Matrix-----[TN FP  FN TP]\")\n",
        "\n",
        "print(metrics.confusion_matrix(predictions_NB_1, Test_Y1))\n",
        "print(metrics.classification_report(predictions_NB_1, Test_Y1))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " ------ Confusion Matrix-----[TN FP  FN TP]\n",
            "[[160  30]\n",
            " [ 21 189]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.84      0.86       190\n",
            "           1       0.86      0.90      0.88       210\n",
            "\n",
            "    accuracy                           0.87       400\n",
            "   macro avg       0.87      0.87      0.87       400\n",
            "weighted avg       0.87      0.87      0.87       400\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDYdBZ7TF97S",
        "colab_type": "text"
      },
      "source": [
        "### Logistic Regression (BOW)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHYOHkp3F_D5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8043bca6-d6d5-4a3e-c815-1c13cf1da6b6"
      },
      "source": [
        "LR = linear_model.LogisticRegression(max_iter=300, random_state = 100)\n",
        "LR.fit(Train_X, Train_Y)\n",
        "predictions_LR = LR.predict(Test_X)\n",
        "\n",
        "print(\"LR Accuracy Score -> \",round(accuracy_score(predictions_LR, Test_Y)*100,2),\"%\")"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LR Accuracy Score ->  84.5 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_1Lp8OzGA2v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d117c745-5929-4599-ccc5-1b5491853a6d"
      },
      "source": [
        "confusion_matrix(predictions_LR, Test_Y)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[166,  32],\n",
              "       [ 30, 172]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuyMBHuwGAmH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "c694d5b8-c03f-467b-9be3-f1a38c124326"
      },
      "source": [
        "print(classification_report(predictions_LR, Test_Y))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.84      0.84       198\n",
            "           1       0.84      0.85      0.85       202\n",
            "\n",
            "    accuracy                           0.84       400\n",
            "   macro avg       0.85      0.84      0.84       400\n",
            "weighted avg       0.85      0.84      0.84       400\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAuUYxHKHL2m",
        "colab_type": "text"
      },
      "source": [
        "### Logistic Regression (TF_IDF)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ouni4FliHQPT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ce9a1b42-a1f8-4246-9748-52af402f632b"
      },
      "source": [
        "LR_1 = linear_model.LogisticRegression(max_iter=300, random_state = 100)\n",
        "LR_1.fit(Train_X1, Train_Y1)\n",
        "predictions_LR_1 = LR_1.predict(Test_X1)\n",
        "\n",
        "print(\"LR Accuracy Score -> \",round(accuracy_score(predictions_LR_1, Test_Y1)*100,2),\"%\")"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LR Accuracy Score ->  81.5 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaZpfzpzHQM0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ac81ba32-d311-4cec-9a58-1201d35da0d0"
      },
      "source": [
        "confusion_matrix(predictions_LR_1, Test_Y1)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[145,  38],\n",
              "       [ 36, 181]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EugAkHmKHQJ_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "f2c03ee9-ce76-4485-9b24-9ca70dae700f"
      },
      "source": [
        "print(classification_report(predictions_LR_1, Test_Y1))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.79      0.80       183\n",
            "           1       0.83      0.83      0.83       217\n",
            "\n",
            "    accuracy                           0.81       400\n",
            "   macro avg       0.81      0.81      0.81       400\n",
            "weighted avg       0.81      0.81      0.81       400\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKfWGsueyDdl",
        "colab_type": "text"
      },
      "source": [
        "### Support Vector Machines (BOW)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOxiwpb6kmc5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3661069c-03a5-4a99-ec2a-8be425052708"
      },
      "source": [
        "SVM = svm.SVC(C = 1.0, kernel='linear', degree = 3, gamma = 'auto')\n",
        "SVM.fit(Train_X, Train_Y)\n",
        "predictions_SVM = SVM.predict(Test_X)\n",
        "\n",
        "print(\"SVM Accuracy Score -> \",round(accuracy_score(predictions_SVM, Test_Y)*100,2),\"%\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM Accuracy Score ->  83.5 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dycCi2fSFKBQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3870449d-f237-4805-bdc6-80fb2b09f5db"
      },
      "source": [
        "confusion_matrix(predictions_SVM, Test_Y)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[165,  35],\n",
              "       [ 31, 169]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJHK3RKVFSOO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "b0de94a9-7f78-4e75-b35a-0402751c451f"
      },
      "source": [
        "print(classification_report(predictions_SVM, Test_Y))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.82      0.83       200\n",
            "           1       0.83      0.84      0.84       200\n",
            "\n",
            "    accuracy                           0.83       400\n",
            "   macro avg       0.84      0.83      0.83       400\n",
            "weighted avg       0.84      0.83      0.83       400\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMxwcc0D5mYS",
        "colab_type": "text"
      },
      "source": [
        "### Support Vector Machines (TF-IDF)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62T2oxwt5lyB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "04401fb4-90e1-4917-cab3-36717ae510a4"
      },
      "source": [
        "SVM_1 = svm.SVC(C = 1.0, kernel='linear', degree = 3, gamma = 'auto')\n",
        "SVM_1.fit(Train_X1, Train_Y1)\n",
        "predictions_SVM_1 = SVM_1.predict(Test_X1)\n",
        "\n",
        "print(\"SVM Accuracy Score -> \",round(accuracy_score(predictions_SVM_1, Test_Y1)*100,2),\"%\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM Accuracy Score ->  80.5 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGvEeFxKFcMk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "53d02359-ef01-4bbd-e1d4-d6e3d7c5e7c6"
      },
      "source": [
        "confusion_matrix(predictions_SVM_1, Test_Y1)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[140,  37],\n",
              "       [ 41, 182]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASKDWFugFlct",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "c1ffbe31-385b-4da0-b0e8-b93720cef16a"
      },
      "source": [
        "print(classification_report(predictions_SVM_1, Test_Y1))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.79      0.78       177\n",
            "           1       0.83      0.82      0.82       223\n",
            "\n",
            "    accuracy                           0.81       400\n",
            "   macro avg       0.80      0.80      0.80       400\n",
            "weighted avg       0.81      0.81      0.81       400\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}